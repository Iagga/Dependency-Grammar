{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Grammar (first assignment NLU)\n",
    "\n",
    "\n",
    "* Student name: Gaia Trebucchi\n",
    "* Student number: 224464\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load `en_core_web_sm` with `spacy.load`. This will return a `Language` object stored as `spacy_nlp` containing all components and data needed to process text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence='I saw the man with the telescope.'\n",
    "sentence1='Gaia brought her cat Costina some delicious food'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1:\n",
    "#### Extract a path of dependency relations from the ROOT to a token. \n",
    "* **input**: A sentence passed as a string \n",
    "* **output**: A dictionary whose keys are all the tokens of the input sentence and the value for each keys is a list of tuples, where the first element of the tuple is a token and the second element is the dependency relation between the token and its head in the sentence parsing. The list, for each key, represents the path from the root of the sentence to the token stored as the key.   \n",
    "\n",
    "First, the sentence is processed through `spacy_nlp(sentence)` to obtain a `Doc` object. Then a dictionary is created to store the dependency relation paths from the root to each token. \n",
    "The function cycles through all the `Token` objects of the `Doc` in this way: \n",
    "* each token is added as first element of its relative `dep_path` in a tuple whose second element is the dependency relation with its head\n",
    "* a while loop adds to the relative path all the tuples (token, dependency relation) that we encounter by ascending  the dependencies from the token we are considering. This step is done by calling the head of the token until we found the root of the sentence. To state that, the stopping criterium for the while loop is a check of the token dependency , if it results equal to `'ROOT'` the function exits the while. Before adding the relative path for each token to the dictionary, the path is reversed to follow the descending order from the root to the token. The choice of first computing the path in the ascending way and then reverse it it's due to the fact that each token owns a unique head, on the contrary it could have more than one child and the search of the descending path in that case would need more than one iteration across the sentence. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_dependency(sentence):\n",
    "    doc=spacy_nlp(sentence)\n",
    "    list_path=dict()\n",
    "    for token in doc:\n",
    "        tok=token\n",
    "        dep_path=[(tok,tok.dep_)]\n",
    "        while tok.dep_!='ROOT':\n",
    "            tok=tok.head\n",
    "            dep_path.append((tok,tok.dep_))\n",
    "        path=dep_path[::-1]\n",
    "        list_path[token]=path\n",
    "    return list_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with the sentence \"I saw the man with the telescope\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I: [(saw, 'ROOT'), (I, 'nsubj')], saw: [(saw, 'ROOT')], the: [(saw, 'ROOT'), (man, 'dobj'), (the, 'det')], man: [(saw, 'ROOT'), (man, 'dobj')], with: [(saw, 'ROOT'), (man, 'dobj'), (with, 'prep')], the: [(saw, 'ROOT'), (man, 'dobj'), (with, 'prep'), (telescope, 'pobj'), (the, 'det')], telescope: [(saw, 'ROOT'), (man, 'dobj'), (with, 'prep'), (telescope, 'pobj')], .: [(saw, 'ROOT'), (., 'punct')]}\n"
     ]
    }
   ],
   "source": [
    "print(path_dependency(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuction 2:\n",
    "#### Extract subtree of a dependents given a token.\n",
    "* **input**: a sentence passed as string\n",
    "* **output**: a dictionary whose keys are all the tokens of the input sentence and whose value for each key is the list of tokens belonging to the subtree of the key token, in the order they appear in the sentence.\n",
    "\n",
    "First, the sentence is processed through `spacy_nlp(sentence)` to obtain a `Doc` object. Then a dictionary is created to store the subtree of each token.\n",
    "The function cycles through all the `Token` objects in the `Doc` in this way:\n",
    "* the subtree of the Token is extracted through the attribute `Token.subtree` that returns the subtree containing the token and all the token's syntactic descendants.\n",
    "* A list is created and each token of the subtree is added\n",
    "* The token is added to the dictionary as key and its subtree list is addes as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtree_token(sentence):\n",
    "    doc=spacy_nlp(sentence)\n",
    "    sub_token=dict()\n",
    "    for token in doc:\n",
    "        depend=[]\n",
    "        sub=token.subtree\n",
    "        for t in sub:\n",
    "            depend.append(t)\n",
    "        sub_token[token]=depend\n",
    "    return sub_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with the sentence \"I saw the man with the telescope\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I: [I], saw: [I, saw, the, man, with, the, telescope, .], the: [the], man: [the, man, with, the, telescope], with: [with, the, telescope], the: [the], telescope: [the, telescope], .: [.]}\n"
     ]
    }
   ],
   "source": [
    "print(subtree_token(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3:\n",
    "#### check if a given list of tokens (segment of a sentence) forms a subtree.\n",
    "* **input**: a sentence passed as a string and a segment passed as a list\n",
    "* **output**: a `Boolean` that states if the segment forms a valid subtree of dependency in the sentence\n",
    "\n",
    "First, the function `subtree_token` defined above (Function 2) is used to extract all the dependency subtrees of the sentence, stored as values of a dictionary.\n",
    "Then, the function cycles across all the values of the dictionary (all the subtrees) in that way:\n",
    "* The first check is on the length of the sentence and the number of elements of the subtree: if they don't match, the function skip to the next subtree without comparing the segment with the subtree components\n",
    "* If the length match, the function creates a list with the text of the subtree tokens and compares it with the segment. If they are equivalent the segment forms a valid subtree and True is returned, otherwise the function proceeds to the successive value. \n",
    "\n",
    "At the end of the for cycle, if there aren't subtree that match the input segment, False is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def check_subtree(sentence,segment):\n",
    "    sub_tree=subtree_token(sentence)\n",
    "    for value in sub_tree.values():\n",
    "        if len(value)==len(segment):\n",
    "            text_list=[]\n",
    "            for token in value:\n",
    "                text_list.append(token.text)\n",
    "            if text_list==segment:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with two different segments (one that forms a subtree of dependencies in the input sentence parsing and one that doesn't) and the sentence \"I saw the man with the telescope\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(check_subtree(sentence,[ 'the', 'man', 'with']))\n",
    "print(check_subtree(sentence,[ 'the', 'man','with','the', 'telescope']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 4:\n",
    "#### identify head of a span, given its tokens.\n",
    "* **input**: a list of tokens (not necessarily a sentence)\n",
    "* **output**: the `Token` representing the head of the span formed by the input tokens\n",
    "\n",
    "First, the function converts the list of tokens in a single string, that is then processed by `spacy_nlp(segment_string)` to produce a `Doc` object. Then a `Span` that covers the entire segment is created from the and the head of the span is returned by `span.root` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_of_span(segment):\n",
    "    segment_string=segment[0]\n",
    "    for i in range(1,len(segment)):\n",
    "        segment_string+=\" \"+segment[i]\n",
    "    doc=spacy_nlp(segment_string)\n",
    "    span=doc[:]\n",
    "    return span.root\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with different lists of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n",
      "chance\n"
     ]
    }
   ],
   "source": [
    "print(head_of_span(['the', 'man','with','the','telescope']))\n",
    "print(head_of_span(['last','chance','for','you']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 5:\n",
    "#### extract sentence subject, direct object and indirect object spans.\n",
    "* **input**: a sentence passed as string\n",
    "* **output**: a dictionary whose keys are tuples consisting of the token and its dependency relation and the value for each key is the span of the token.\n",
    "\n",
    "First, the sentence is processed with `spacy_nlp(sentence)` to obtain a `Doc` object and a dictionary is created to store the dependency we are focusing on and the relative spans. \n",
    "The function cycles through all the `Token` object of the `Doc` in this way:\n",
    "* The first step consists of stating if the dependency of the token under observation matches one of the dependency we are interested in (the dependency should be `'nsubj'` for the subject, `'dobj'` for the direct object and `'dative'` for the indirect object).\n",
    "* When a subject, a direct object or a indirect object token is found the function creates a `Span` object of the subtree having the token as root. This is done by using the attribute `token.left_edge.i` and `token.right_edge.i` that provides respectively the index of the first and the index of the last token of the subtree, that are used to create a `Span` object for the syntactic phrase we are interested in.\n",
    "* The last step consists of adding to the dictionary a tuple (token, dependency of the token) as key and the span found at the previous step as value. The choice of storing as key this tuple has been made to highlight the head of the span (i.e. the token having as dependency 'nsubj', 'dobj' or 'dative').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(sentence):\n",
    "    doc=spacy_nlp(sentence)\n",
    "    spans_dict=dict()\n",
    "    for token in doc:\n",
    "        if token.dep_=='nsubj' or token.dep_==\"dobj\" or token.dep_==\"dative\":\n",
    "            span=doc[token.left_edge.i:token.right_edge.i+1]\n",
    "            spans_dict[(token,token.dep_)]=span\n",
    "    return spans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with the two sentences: \"I saw the man with the telescope\", \"Gaia brought her cat Costina some delicious food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(I, 'nsubj'): I, (man, 'dobj'): the man with the telescope}\n",
      "{(Gaia, 'nsubj'): Gaia, (Costina, 'dative'): her cat Costina, (food, 'dobj'): some delicious food}\n"
     ]
    }
   ],
   "source": [
    "print(get_spans(sentence))\n",
    "print(get_spans(sentence1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
